---
title: "2018 Lynn Canal MAGMA Data Cleanup and Prep"
date: "01/28/2019"
editor_options:
  chunk_output_type: inline
output:
  html_document:
    df_print: paged
subtitle: 
creator: Chase Jalbert
---


```{r libraries, message = FALSE}
if (!require("pacman")) install.packages("pacman"); library(pacman) # install pacman, if not installed
  p_load(tidyverse) # use pacman to load or install+load necessary packages
```

<b> Project Overview </b> 

This is a post-season analysis of Lynn Canal D115 fishery. I want an age-specific stock composition for all major contributing age classes (>5%; 1.2, 1.3, 2.2, 2.3, other) using mark- and age-enhanced genetic mixed-stock analysis (MAGMA).    

For the purposes of this project, I am just concerend about the breakdown by age, across all statweeks, and statareas. I will assign <b>all</b> stat areas as 115-<i>00</i>, since this doesnt exist and is currently how stat area is recorded.    
 
Total season estimates are to be provided by age group. The algorithm will be run for 40 000 repititions, and the first 20 000 repititions will be discarded.    

Deliverables: 
    
    Point estimates, Credibility intervals
<b> Script Overview </b>

The purpose of this script is to prepare the MAGMA data. Here I am going to work thorugh catch, harvest, and ASL data and output the MAGMA required format. I will carry out a bit of data QA and fix any outstanding data issues.    

After this, I will run the MAGMA setup script to setup the MAGMA workspace and perform analysis. 


<b> Begin Data Cleanup </b>

### Make MAGMA directory structure ###

Note: dir.create() is nice because it doesn't overwrite existing directories. If it already exists you'll simply get a warning.

```{r create_dirs}
dir.create(file.path("../2018/", "MAGMA")) # create MAGMA directory. 

dir.create(file.path("../2018/MAGMA", "analysis")) # create analysis subdir. 

dir.create(file.path("../2018/MAGMA", "data")) # create data subdir. 

dir.create(file.path("../2018/MAGMA", "output")) # create output subdir. 
```

### 1) Load ASL Metadata ###

First, load the metadata, which was exported from OceanAK and perform QA.     

I will rearrange, check missingness, verify genetic info, and check for dupoliocate entries since we had the whatman card isue with leading zeros vs 1xxxxx####.

```{r prepdata}
metadata <-
  read_csv(file = "../2018/Detailed ASL Samples_011519.csv")

unique(metadata$`Stat Area`) # gives unique values for subdistricts.

metadata <- metadata %>%
  mutate(stat_area = "115-00") # assign all samples as 115-00

unique(metadata$stat_area) # verify no NAs remain and all stat areas look okay or are as expected.

sum(is.na(metadata$`Dna Specimen No`)) # Check for missing DNA data, should be 0 if no missing data 

sum(!is.na(metadata$`Thermal Mark Id`)) # Check for hatchery fish in the sample.
```
#### Detect duplicate whatman cards
```{r dup_check}
duplicate_cards <- metadata %>% 
  group_by(`Dna Specimen No`) %>% 
  filter( n() > 1) # verify that there are no duplicate whatmancard_positions (i.e., duplicate cards since DNA Specimen No is a concatenation of Whatman card number and fish number. 

duplicate_cards # Check the duplicate list. If zero then congrats, you can pass go and collect $200, or whatever. 

rm(duplicate_cards) # remove duplicate object since we have fixed any issues by now. 
```

#### Check out the data since its loaded and cleaned up. Also, see if we can identify any issues.
```{r Visualization}
metadata %>%
  filter(!is.na(`Dna Specimen No`)) %>%
  ggplot(aes(x = `Stat Week`, fill = District)) +
  geom_bar() +
  scale_x_continuous( breaks = metadata$`Stat Week`, labels = metadata$`Stat Week`) +
  ylab("# DNA Samples") +
  ggtitle("Samples by Stat Week for District 115")
```


I'll make a table for the total DNA samples in each district, for each statisical week.    From here, I'll calculate proportions of the total harvest by week for each area. 

```{r ASL_import}
ASL <- metadata %>% 
  select(c(`Stat Week`, stat_area, `Dna Specimen No`, `Age European`, `Thermal Mark Id`, Year)) %>%  # subset stat week and location information
  separate(stat_area, "-(?=[^-]*$)",
           into = c("district", "subdistrict"),
           remove = TRUE) %>%  # Split location information at the hyphen and remove the old 'stat_area' column
  unite(statarea, c(district, subdistrict), sep = "-", remove = FALSE)
```


Now, let's create a few summary tables so we can see what we're looking at. The summaries will be by stat week ("SW") and by District.    

 The total number of samples avaialble:
```{r total_avail_samples}
total_samples <- ASL %>% 
  group_by( `Stat Week`, statarea) %>% 
  summarize( n = n()) %>% 
  ungroup() %>% 
  spread( statarea, n, fill = 0) # Total number of samples by stat week

total_samples %>% 
  select(-`Stat Week`) %>% 
  summarize_all( funs(sum)) # Total number of samples for each stat area (this case just -00)

sum(total_samples[-(1)]) # Grand total of samples, first column (stat week) is removed for calculations. 
```

For MAGMA, I need a specific tab-delimited format as shown here:
    
    SILLY_VIAL	YEAR	STAT_WEEK	DISTRICT	SUBDISTRICT	AGE_EUROPEAN	SOURCE    
    SGILL17D6_831801	2017	25	106	41	13	WILD    
    SGILL17D6_831802	2017	25	106	41	13	TAHL    
    SGILL17D6_831803	2017	25	106	41	13	WILD    
    SGILL17D6_831804	2017	25	106	41	NA	WILD 

The next few lines will create the fields and forms necessary. Briefly, I will make the SILLY code, pull fishery information (SW, DIST, SUB), and age. Finally, I will add the fish number (VIAL), which is done via a LOKI lookup. 

```{r build_SILLY}
# Build columns containing parts of the silly. Note to future self: The code probably fails on Districts with zeros (e.g., 106). In our example, the silly would be SGILL18D06, instead of just D6. Would need to toss in a str_remove or something similar to fix. In this case I just have D115 so don't really care for the extra line.

ASL$SILLY <- paste0(
  "SGILL",
  str_sub(ASL$Year, start = 3, end = 4),
  "D" ,
  str_sub(metadata$stat_area, start = 2, end = 3)
)
```

Now, import the LOKI genetic data so I can pair FISHID. For this I will have to create the Dna Specimen No, similar to the ASL data, then join FK_FISH_ID back to the metadata. I'll turn this into VIAL for the SILLY_VIAL input column. 

```{r read_loki}
loki <-
  read_csv(file = "../2018/GEN_SAMPLED_FISH_TISSUE_011719.csv") %>%
  select( c(FK_FISH_ID, DNA_TRAY_CODE, DNA_TRAY_WELL_CODE, CAPTURE_DATE))
```

First carry out a bit of doublechecking to make sure there is nothing goofy going on with the card numbers.    

Unfortunately there are a mix of 100000XXXX and 000000XXXX WGCs in this year's samples. `Dna Specimen No` isn't enough for me to figure out the whole 10 digit WGC number.    

I need to check and verify that there are no potential "duplicate" cards (i.e. cards with the same last 4 digits). If there arent then continue. If I find 'duplicates' then I'll have to dive into the datasheets and figure out what's what.
```{r loki_dups}
loki %>% 
  mutate(WGC_4digit = str_sub(string = DNA_TRAY_CODE, start =  -4)) %>%  # get last 4 digits of WGC
  group_by(WGC_4digit) %>%  # group by those 4 digits
  summarise(count = n_distinct(CAPTURE_DATE)) %>%  # count unique sample dates
  summarise(count = max(count)) # what is the maximum number of sample dates per unique 4 digit WGC, should be 1. If not, proceed to testing / fixing section. Do not pass go, do not collect $200. 
```

There are no duplicates, so I'll move forward with the join to obtain FISH ID!

```{r loki_join}
loki$DNA_TRAY_WELL_CODE <-
  sprintf("%02d", as.numeric(loki$DNA_TRAY_WELL_CODE)) # pad all single digit whatman card spots with zeros (e.g., 1 = 01).

loki <- loki %>%
  mutate(WGC_4digit = str_sub(string = DNA_TRAY_CODE, start =  -4)) %>% 
  unite(
    `Dna Specimen No`,
    c(WGC_4digit, "DNA_TRAY_WELL_CODE"),
    sep = "",
    remove = TRUE
  ) %>%
  select(-DNA_TRAY_CODE)

loki$`Dna Specimen No` <-
  as.numeric(loki$`Dna Specimen No`) # convert to numeric so join is acting on same type of data

ASL_genetic <-
  left_join(x = ASL, y = loki, by = "Dna Specimen No") # join by Dna Specimen No, leaving FISHID (VIAL)
```

Someting seems a bit off here. Our genetic data contains 3410 observations, while the ASL metadata contains 3460.... I need to investigate to see why there are 50 missing samples. 

```{r loki_missing}
(missing <- ASL_genetic %>%
   filter(is.na(FK_FISH_ID))) # display samples that appear in the ASL data but not the genetic dataset. 
```
It looks like there are two cards, one 40 (xxxxxx9167) and one 10 (xxxxxx1256) making up these 'missing' fish.    
Further investigation shows these cards are not in the sample shipment inventory and were not built in the genetic database. This means that they were sampled for ASL+genetics but never made it to HQ for shipment... We have no way to trace them so they will be treated as <i> age only </i> fish. Hence, I am removing the DnaSpecimenNo from those samoples for further analyses. 

```{r final_asl}
ASL_genetic_clean <-
  ASL_genetic[!is.na(ASL_genetic$FK_FISH_ID), ] # remove the missing cards.

rm(list = c("missing", "ASL_genetic", "loki", "ASL", "metadata")) # Cleanup unnecessary dataframes.
```


Now I will build the magma metadate in the required input format.    
  
```{r maga_metadata}
MAGMA <- ASL_genetic_clean %>%
  unite(SILLY_VIAL,
        c(SILLY, FK_FISH_ID),
        sep = "_",
        remove = TRUE) %>%
  select (-c (statarea, `Dna Specimen No`, CAPTURE_DATE)) # remove unncessary columns; used earlier for troubleshooting/ pairing

MAGMA_meta <- MAGMA %>%
  rename(
    YEAR = Year,
    STAT_WEEK = `Stat Week`,
    AGE_EUROPEAN = `Age European`,
    SOURCE = `Thermal Mark Id`, 
    DISTRICT = district,
    SUBDISTRICT = subdistrict
  ) %>%
  mutate(SOURCE = replace_na(SOURCE, "WILD")) %>% # replace all NAs with wild, which should be all fish in this case...
  select(SILLY_VIAL, YEAR, everything()) # Put it all in the correct order

MAGMA_meta$SOURCE <-
  str_sub(MAGMA_meta$SOURCE, start = 1, end = 4) # only leave first four letters of source
```

```{r metadata_export}
write_tsv(x = MAGMA_meta, path = "../2018/MAGMA/data/metadata.txt", na = "NA", append = FALSE, col_names = TRUE, quote_escape = "double")
```



#### 2) Harvest Data ####

I'm going to load in the harvest data for the 2018 season so I can check otut he harvest by district and by stat week. The ultimate goal is to create a summary table of harvest at each district - subdistrict for each statweek.   

For example: 
    
    YEAR	DISTRICT	SUBDISTRICT	STAT_WEEK	HARVEST
    2018	106	30	25	50
    2018	106	30	26	613
    2018	106	30	27	860
    2018	106	30	28	750

```{r read_harvest}
harvest_raw <-
  read_csv(file = "../../Lynn Canal Inseason/2018/Harvest/ft - Detailed Fish Tickets.csv") 
```

Check out the data since its loaded and cleaned up. Also, see if we can identify any issues.    
```{r harvest_visual}
harvest_raw %>% 
  ggplot(aes(x = `Stat Week`, y = `Number Of Animals (sum)` , fill = District )) +
  geom_col() +
  scale_x_continuous( breaks = harvest_raw$`Stat Week`, labels = harvest_raw$`Stat Week`) +
  ylab("Total Sockeye Harvested") +
  ggtitle("Catch by Stat Week for District 115")
```

Great, but it looks like we have a fair bit of harvest after week 34. Since I only have collection information to SW34, I am going to end the analysis here. Any age-comps after this will be projections and will be up to the managers.    

Now, create a table of harvest by statweek and subdistrict.    

```{r}
MAGMA_harv <- harvest_raw %>%
  select(c(Year, `Stat Area`, `Stat Week`, `Number Of Animals (sum)`)) %>%  # keeping only the important columns.
  mutate(DISTRICT = str_sub(harvest_raw$`Stat Area`, start = 1, end = 3), # just want the first three values of district (115)
         SUBDISTRICT = "00") %>% # hard code subdistrict, b/c no -00 but we want to consider this fishery as one. 
  rename(STAT_WEEK = `Stat Week`,
         YEAR = Year,
         HARVEST = `Number Of Animals (sum)`) %>%
  select(YEAR, DISTRICT, SUBDISTRICT, STAT_WEEK, HARVEST,-`Stat Area`)  %>% # select only the important columns
  filter(STAT_WEEK < 35) %>% # only considering SW 24-34 due to available data. 
  group_by(YEAR, DISTRICT, SUBDISTRICT, STAT_WEEK, STAT_WEEK) %>% # grouping to get summary harvest for each week
  summarize(HARVEST = sum(HARVEST)) %>% # summary of harvest for each stat week
  ungroup() # ungroup to get back to the by week list
``` 

```{r harvest_export}
write_tsv(x = MAGMA_harv, path = "../2018/MAGMA/data/harvest.txt", na = "NA", append = FALSE, col_names = TRUE, quote_escape = "double")
```


### Groups.txt and group_names.txt setup ###

Now, I need to pull the new (2018) baseline information and update the groups.txt and group_names.txt files for MAGAMA. 

```{r prep_txt}
groups7_pops239 <-
  read_csv(file = "../../Lynn Canal Inseason/2018/Lynn Canal Inseason 7 Reporting Groups 239 Poupluations 2018.csv") # load in updated baseline info


(unique(groups7_pops239$Reporting.Group.New)) # view current reporting groups.


groups4_pops239 <- groups7_pops239 %>%
  mutate(
    GROUPS = case_when(
      Reporting.Group.New == "Snettisham" ~ "Other",
      Reporting.Group.New == "Juneau Mainland" ~ "Other",
      Reporting.Group.New == "Taku River/Stikine Mainstem" ~ "Other",
      TRUE ~ as.character(Reporting.Group.New)
    )
  ) %>% # changing all unnecessary groups to "Others" for MAGMA
  mutate(
    D115 = case_when(
      GROUPS == "Other" ~ 1,
      GROUPS == "Chilkat Mainstem" ~ 2,
      GROUPS == "Chilkat Lake" ~ 3,
      GROUPS == "Chilkoot" ~ 4
    )
  )

group_names <- 
  as.data.frame(unique(groups4_pops239$GROUPS)) # find all unique groups, should be 4...

group_names <- group_names %>% 
  rename(D115 = `unique(groups4_pops239$GROUPS)`) # rename for MAGMA

groups <- groups4_pops239 %>%
  select(c(SILLY, D115)) # select only important columns

groups <- groups %>%
  rename(SOURCE = SILLY) # rename for MAGMA
```


```{r export_txt}
write_tsv(x = group_names, path = "../2018/MAGMA/data/group_names.txt", na = "NA", append = FALSE, col_names = TRUE, quote_escape = "double") # export object group_names

write_tsv(x = groups, path = "../2018/MAGMA/data/groups.txt", na = "NA", append = FALSE, col_names = TRUE, quote_escape = "double")
```

Great, now I can move onto the MAGMA_setup script to prepare the MAGMA workspace and run the model.    

In the interest of record keeping: 

```{r}
sessionInfo()
```

